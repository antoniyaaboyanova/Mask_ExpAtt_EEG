{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "import pickle\n",
    "\n",
    "# load extra 53 concepts \n",
    "path = \"./Animacy/things_concepts.tsv\"\n",
    "df = pd.read_csv(path, sep=\"\\t\")\n",
    "\n",
    "# extract \n",
    "image_path = \"/projects/archiv/DataStore_Boyanova/ExpAtt_EEG/Image_dataset/Images\"\n",
    "extra_concepts = df.columns[2:].to_numpy()\n",
    "concept_mat = np.isin(df.iloc[0:-1, 2:].to_numpy(), 1)\n",
    "defenitions = df[\"Definition (from WordNet, Google, or Wikipedia)\"][:-1].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "REORDER_RULES = {\n",
    "    \"camera\": [\"camera_lens\", \"camera1\", \"camera2\"],\n",
    "    \"chicken\": [\"chicken_wire\", \"chicken1\", \"chicken2\"],\n",
    "    \"crystal\": [\"crystal_ball\", \"crystal1\", \"crystal2\"],\n",
    "    \"hot\": [\"hot_chocolate\", \"hot_tub\", \"hot-air_balloon\", \"hot-water_bottle\"],\n",
    "    \"ice\": [\"ice_cream\", \"ice_cube\", \"ice_pack\", \"ice-cream_cone\"],\n",
    "    \"pepper\": [\"pepper_mill\", \"pepper1\", \"pepper2\"],\n",
    "}\n",
    "\n",
    "def reorder_categories_inplace(categories_os):\n",
    "    categories = list(categories_os)  # copy\n",
    "\n",
    "    for key, desired_order in REORDER_RULES.items():\n",
    "        # indices where any of the target categories occur\n",
    "        indices = [i for i, c in enumerate(categories) if c in desired_order]\n",
    "\n",
    "        if not indices:\n",
    "            continue\n",
    "\n",
    "        # keep only those present, in desired order\n",
    "        reordered = [c for c in desired_order if c in categories]\n",
    "\n",
    "        # replace in the original span\n",
    "        for idx, new_cat in zip(sorted(indices), reordered):\n",
    "            categories[idx] = new_cat\n",
    "\n",
    "    return categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_os = sorted(os.listdir(image_path))\n",
    "categories_os = reorder_categories_inplace(categories_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Prompt template for animacy\n",
    "def make_prompt(concept_name, definition):\n",
    "    return f\"\"\"\n",
    "Classify the following concept as either animate or inanimate based on the specific rules below.\n",
    "\n",
    "### RULES:\n",
    "- Animacy is a term which describes if a given concept is capable of self-movement. \n",
    "- Animate: Only includes humans (including human body parts) and animals (mammals, birds, fish, insects). These entities have self-directed movement and agency.\n",
    "- Inanimate: Includes all physical objects, man-made tools, materials, and ALL vegetation.\n",
    "- CRITICAL: Plants, trees, and flowers are biologically alive but must be classified as INANIMATE for this task.\n",
    "\n",
    "### EXAMPLES:\n",
    "Concept: Calf is always animate\n",
    "Concept: Hammer -> inanimate\n",
    "Concept: Oak tree -> inanimate\n",
    "Concept: Grass -> inanimate\n",
    "Concept: Human -> animate\n",
    "Concept: Mosquito -> animate\n",
    "Concept: Apple tree -> inanimate\n",
    "\n",
    "### TASK:\n",
    "Concept: {concept_name}\n",
    "Defenition: {definition}\n",
    "\n",
    "Answer with exactly one word (animate or inanimate):\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# Classification function\n",
    "# -----------------------------\n",
    "def classify_animacy(concepts_batch):\n",
    "    prompts = [make_prompt(c) for c in concepts_batch]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=3,   # enough for \"animate\" or \"inanimate\"\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode outputs\n",
    "    results = []\n",
    "    for i, out in enumerate(outputs):\n",
    "        decoded = tokenizer.decode(out, skip_special_tokens=True).strip().lower()\n",
    "        # extract the first occurrence of \"animate\" or \"inanimate\"\n",
    "        if \"animate\" in decoded:\n",
    "            results.append(\"animate\")\n",
    "        elif \"inanimate\" in decoded:\n",
    "            results.append(\"inanimate\")\n",
    "        else:\n",
    "            results.append(\"unknown\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def dump_data(data, filename):\n",
    "    \"\"\"\n",
    "    Serializes and saves data to a file using pickle.\n",
    "    ------\n",
    "    Args:\n",
    "        data (any): The data to be serialized and saved.\n",
    "        filename (str): The path to the file where the data will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boyanova/miniconda3/envs/llama_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# LLama-3\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "cache_pt =\"/projects/crunchie/boyanova/EEG_Things/Grouping-Embeddings/00_stim_prep/models\"\n",
    "# 1. Configure 4-bit quantization to save memory\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# 2. Load the model with device_map=\"auto\"\n",
    "# This automatically handles the memory split between GPU and CPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_pt,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=cache_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force the tokenizer to use EOS as the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Update the model's configuration so it knows about this change\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import transformers\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# This silences the specific Transformers warnings\n",
    "transformers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1854/1854 [07:12<00:00,  4.29it/s]\n"
     ]
    }
   ],
   "source": [
    "animate_llama = []\n",
    "for c, d in tqdm(zip(categories_os, defenitions), total=len(categories_os)):\n",
    "    # 1. Format the prompt using Llama 3.1's chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": make_prompt(c, d)},\n",
    "    ]\n",
    "\n",
    "    # This turns your text into the special format Llama 3.1 expects\n",
    "    input_prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 2. Generate the response\n",
    "    # We add a 'terminator' to make sure the model stops at the right time\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_prompt,\n",
    "        max_new_tokens=512,      # How long you want the answer to be\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,          # Set to True for creative answers\n",
    "        temperature=0.5,         # Lower is more focused, higher is more creative\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    # 3. Decode and print the result\n",
    "    response = outputs[0][input_prompt.shape[-1]:] # This removes the input prompt from the output\n",
    "    animate_llama.append(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_llama = np.array(animate_llama)\n",
    "\n",
    "animate_llama[np.isin(categories_os, \"calf2\")] = \"animate\"\n",
    "\n",
    "llama_animate = np.array(categories_os)[animate_llama == \"animate\"]\n",
    "llama_inanimate = np.array(categories_os)[animate_llama == \"inanimate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\"categories\": categories_os,\n",
    "             \"animate_llama\": animate_llama,\n",
    "             \"path\": [os.path.join(image_path, cat) for cat in categories_os]}\n",
    "df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_data(data_dict, \"./Animacy/llama_animacy.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1854/1854 [00:00<00:00, 24281.40it/s]\n"
     ]
    }
   ],
   "source": [
    "inan = 1\n",
    "an = 1\n",
    "for idx in tqdm(range(len(df))):\n",
    "    data = df.iloc[idx]\n",
    "    files = len(os.listdir(data.path))\n",
    "    if data.animate_llama == \"animate\":\n",
    "        an = an + files\n",
    "    else:\n",
    "        inan = inan + files\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
