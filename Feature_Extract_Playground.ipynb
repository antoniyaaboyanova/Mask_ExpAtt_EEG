{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boyanova/miniconda3/envs/feature_extract/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from thingsvision import get_extractor\n",
    "from thingsvision.utils.storing import save_features\n",
    "from thingsvision.utils.data import ImageDataset, DataLoader\n",
    "from thingsvision.core.extraction import center_features\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from typing import Any, Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path/to/images \n",
    "full_image_path = \"/projects/archiv/DataStore_Boyanova/ExpAtt_EEG/Image_dataset/Images_plants\" \n",
    "category = full_image_path.split(\"/\")[-1].split(\"_\")[1]\n",
    "\n",
    "# path/to/output  \n",
    "full_output_path =  f\"/projects/archiv/DataStore_Boyanova/ExpAtt_EEG/Image_dataset/features_{category}\"  \n",
    "os.makedirs(full_output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "                    extractor: Any,\n",
    "                    module_name: str,\n",
    "                    image_path: str,\n",
    "                    out_path: str,\n",
    "                    batch_size: int,\n",
    "                    flatten_activations: bool,\n",
    "                    apply_center_crop: bool,\n",
    "                    class_names: List[str]=None,\n",
    "                    file_names: List[str]=None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract features for a single layer.\"\"\"                                    \n",
    "    dataset = ImageDataset(\n",
    "        root=image_path,\n",
    "        out_path=out_path,\n",
    "        backend=extractor.get_backend(),\n",
    "        transforms=extractor.get_transformations(resize_dim=256, crop_dim=224),\n",
    "        class_names=class_names,\n",
    "        file_names=file_names,\n",
    "    )\n",
    "    batches = DataLoader(dataset=dataset, batch_size=batch_size, backend=extractor.get_backend())\n",
    "    features = extractor.extract_features(\n",
    "                    batches=batches,\n",
    "                    module_name=module_name,\n",
    "                    flatten_acts=flatten_activations,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # save features to disk\n",
    "    save_features(features, out_path=f'{out_path}/raw_{model_name}_{module_name}', file_format='npy')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "\n",
      "['V1.conv1', 'V1.conv2', 'V2.conv_input', 'V2.skip', 'V2.conv1', 'V2.conv2', 'V2.conv3', 'V4.conv_input', 'V4.skip', 'V4.conv1', 'V4.conv2', 'V4.conv3', 'IT.conv_input', 'IT.skip', 'IT.conv1', 'IT.conv2', 'IT.conv3']\n"
     ]
    }
   ],
   "source": [
    "pretrained = True # use pretrained model weights\n",
    "model_path = None # if pretrained = False (i.e., randomly initialized weights) set path to model weights\n",
    "batch_size = 32 # use a power of two (this can be any size, depending on the number of images for which you aim to extract features)\n",
    "apply_center_crop = True # center crop images (set to False, if you don't want to center-crop images)\n",
    "flatten_activations = True # whether or not features (e.g., of Conv layers) should be flattened\n",
    "class_names = None  # optional list of class names for class dataset\n",
    "file_names = None # optional list of file names according to which features should be sorted\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# alexnet, vgg16, resnet50 source = 'torchvision\n",
    "# cornet-s and cornet-rt source = 'custom'  \n",
    "model_name = 'cornet-s'\n",
    "source = 'custom'\n",
    "extractor = get_extractor( \n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained,\n",
    "            model_path=model_path,\n",
    "            device=device,\n",
    "            source=source)\n",
    "\n",
    "# custom extraction // \n",
    "# after every layer extraction + pca result is saved in a folder in the output_dir\n",
    "module_names = []\n",
    "for name, layer in extractor.model.named_modules(): \n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        module_names.append(name)\n",
    "        \n",
    "print(module_names)\n",
    "if model_name == \"cornet-s\":\n",
    "    module_names = ['V1.conv1',  'IT.conv3']\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IT.conv3'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier.1', 'classifier.4', 'classifier.6']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = True # use pretrained model weights\n",
    "model_path = None # if pretrained = False (i.e., randomly initialized weights) set path to model weights\n",
    "batch_size = 32 # use a power of two (this can be any size, depending on the number of images for which you aim to extract features)\n",
    "apply_center_crop = True # center crop images (set to False, if you don't want to center-crop images)\n",
    "flatten_activations = True # whether or not features (e.g., of Conv layers) should be flattened\n",
    "class_names = None  # optional list of class names for class dataset\n",
    "file_names = None # optional list of file names according to which features should be sorted\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# alexnet, vgg16, resnet50 source = 'torchvision\n",
    "# cornet-s and cornet-rt source = 'custom'  \n",
    "model_name = 'cornet-s'\n",
    "source = 'custom'\n",
    "extractor = get_extractor( \n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained,\n",
    "            model_path=model_path,\n",
    "            device=device,\n",
    "            source=source)\n",
    "\n",
    "# custom extraction // \n",
    "# after every layer extraction + pca result is saved in a folder in the output_dir\n",
    "module_names = []\n",
    "for name, layer in extractor.model.named_modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "            module_names.append(name)\n",
    " \n",
    "# in case your code breaks for some reason \n",
    "# when you restart you can adjust from which layer \n",
    "# by picking the elements from the list module_names  \n",
    "for idx, l in enumerate(module_names):\n",
    "  module_name = l\n",
    "  print(module_name)\n",
    "  print('Layer ', idx, 'out of', len(module_names))\n",
    "  print(\"-\" * 20) \n",
    "  \n",
    "  extract_features(\n",
    "                    extractor = extractor,\n",
    "                    module_name =  module_name,\n",
    "                    image_path=full_image_path,\n",
    "                    out_path=full_output_path,\n",
    "                    batch_size=batch_size,\n",
    "                    flatten_activations=flatten_activations,\n",
    "                    apply_center_crop=apply_center_crop,\n",
    "                    class_names=class_names,\n",
    "                    file_names=file_names,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature_extract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
